<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Gradient Descent</title>
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<title>andersch.dev</title>
<link rel="icon" type="image/x-icon" href="/favicon.ico">
<link rel="stylesheet" href="/style.css">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ubuntu:regular,bold&subset=Latin"><script type="text/javascript" src="/script.js" defer></script>
</head>
<body>
<header id="top" class="status">
<header>
<h1 class="title" id="title"><a href="/">andersch.dev</a></h1>

<div class="icons">
  	<a id="icon-github" href="https://github.com/dandersch" ><span>dandersch</span><img src="/icons/github.svg"></a>
  	<a id="icon-feed"   href="https://andersch.dev/feed.xml" target="_blank"><img src="/icons/rss.svg"><span></span></a>
  	<a id="icon-mail"   href="mailto:contact [at} andersch {dot) dev"  ><img src="/icons/mail.svg"><span>contact [at} andersch {dot) dev</span></a>
</div>
<div>
	<nav class="nav">
		<a class="nav-link" href="/article">article</a>
		<a class="nav-link" href="/project">project</a>
		<a class="nav-link" href="/other">other</a>
		<a class="nav-link" href="/notes">wiki</a>
	</nav>
</div>
</header>
</header>
<main id="content" class="content">
<div class="tags-date-box"><div class="tags"><code>[ <a href="/tag/ai.html">ai</a> <a href="/tag/math.html">math</a> ]</code></div></div><h1>Gradient Descent</h1>
<p>
Gradient descent is an optimization algorithm used to minimize a <a href="cost_function.html#ID-7ae09725-e178-4a36-988a-e9e52ee4eca7">cost function</a>
by iteratively adjusting its parameters in the direction of the steepest
descent, as indicated by the negative gradient of the function.
</p>

<p>
After picking initial parameters and calculating the gradient (rate of change),
small changes of the parameters are performed that reduce the loss. By applying
this process iteratively, the loss function can converge to a local minimum.
</p>

<p>
Gradient descent is widely used in <a href="machine_learning.html#ID-1842c07d-7dc6-437a-833f-0d2779acaca2">machine learning</a>, deep learning, and
statistics.
</p>
<div id="outline-container-Learning%20Rate" class="outline-2">
<h2 id="Learning%20Rate"><a href="#Learning%20Rate">Learning Rate</a></h2>
<div class="outline-text-2" id="text-Learning%20Rate">
<p>
The learning rate in machine learning is a factor that you multiply with the
gradient when training the model (i.e. performing gradient descent).
</p>

<p>
It determines the size of the steps taken towards the minimum of the cost
function during the optimization process (i.e. how much to update the weights).
</p>
</div>
</div>
<div id="outline-container-Methods%20for%20Getting%20the%20Gradient" class="outline-2">
<h2 id="Methods%20for%20Getting%20the%20Gradient"><a href="#Methods%20for%20Getting%20the%20Gradient">Methods for Getting the Gradient</a></h2>
<div class="outline-text-2" id="text-Methods%20for%20Getting%20the%20Gradient">
<p>
Algebraic approach
</p>
<ul class="org-ul">
<li>Symbolically manipulate cost function to get derivative (using <a href="derivate.html#ID-05cfd905-198e-42fb-a4a9-68577a675dea">rules</a>)</li>
<li>✅ Perfect accuracy</li>
<li>❌ Not feasible with long cost functions in practice</li>
</ul>

<p>
Finite Differences (easy, numerical approach)
</p>
<ul class="org-ul">
<li>gradient = (cost(x + epsilon) - cost(x))/epsilon</li>
<li>✅ Internals of cost function don't need to be considered</li>
<li>❌ Very slow to evaluate at many locations</li>
<li>❌ Only approximates derivates and requires tuning an epsilon</li>
</ul>

<p>
Dual Numbers
</p>
<ul class="org-ul">
<li>✅ Exact derivatives</li>
<li>❌ y = f(x) needs to be templated to take dual numbers</li>
<li>❌ Also very slow, and takes a lots of memory. One float per derivative.</li>
</ul>

<p>
<a href="backpropagation.html#ID-ae3d52ca-df3e-46c9-bf9c-5359ae842b15">Backpropagation</a> (hard)
</p>
<ul class="org-ul">
<li>✅ Very fast, exact derivatives</li>
<li>❌ Needs knowledge of y = f(x) to apply chain rule at each step</li>
</ul>
</div>
</div>
<div id="outline-container-Methods%20for%20Performing%20Gradient%20Descent" class="outline-2">
<h2 id="Methods%20for%20Performing%20Gradient%20Descent"><a href="#Methods%20for%20Performing%20Gradient%20Descent">Methods for Performing Gradient Descent</a></h2>
<div class="outline-text-2" id="text-Methods%20for%20Performing%20Gradient%20Descent">
<ul class="org-ul">
<li><i>Stochastic Gradient Descent (SGD)</i>: Updates parameters using the gradient
computed from a single or a few training examples, which introduces noise and
can help escape local minima.</li>

<li><i>RMSprop (Root Mean Square Propagation)</i>: modifies SGD by adapting the learning
rate for each parameter based on the average of recent magnitudes of the
gradients, which helps to stabilize the updates and improve convergence.</li>

<li><i>Adam (Adaptive Moment Estimation)</i>: Combines ideas from both RMSprop and
momentum by maintaining a moving average of both the gradients and the squared
gradients, allowing for adaptive learning rates and faster convergence.</li>
</ul>
</div>
</div>
<div id="outline-container-Code%20Examples" class="outline-2">
<h2 id="Code%20Examples"><a href="#Code%20Examples">Code Examples</a></h2>
<div class="outline-text-2" id="text-Code%20Examples">
<div class="org-src-container">
<pre class="src src-C"><span class="org-preprocessor">#include</span> <span class="org-string">&lt;stdio.h&gt;</span>

<span class="org-type">double</span> <span class="org-function-name">mean_squared_error</span>(<span class="org-type">double</span> *<span class="org-variable-name">x</span>, <span class="org-type">double</span> *<span class="org-variable-name">y</span>, <span class="org-type">double</span> <span class="org-variable-name">m</span>, <span class="org-type">double</span> <span class="org-variable-name">b</span>, <span class="org-type">int</span> <span class="org-variable-name">n</span>) {
    <span class="org-type">double</span> <span class="org-variable-name">mse</span> = 0.0;
    <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">i</span> = 0; i &lt; n; i++) {
        <span class="org-type">double</span> <span class="org-variable-name">prediction</span> = m * x[i] + b; <span class="org-comment-delimiter">// </span><span class="org-comment">y = mx + b</span>
        mse += (y[i] - prediction) * (y[i] - prediction);
    }
    <span class="org-keyword">return</span> mse / n; <span class="org-comment-delimiter">// </span><span class="org-comment">Return average</span>
}

<span class="org-type">void</span> <span class="org-function-name">gradientDescent</span>(<span class="org-type">double</span> *<span class="org-variable-name">x</span>, <span class="org-type">double</span> *<span class="org-variable-name">y</span>, <span class="org-type">double</span> *<span class="org-variable-name">m</span>, <span class="org-type">double</span> *<span class="org-variable-name">b</span>, <span class="org-type">int</span> <span class="org-variable-name">n</span>, <span class="org-type">double</span> <span class="org-variable-name">learningRate</span>, <span class="org-type">int</span> <span class="org-variable-name">iterations</span>) {
    <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">iter</span> = 0; iter &lt; iterations; iter++) {
        <span class="org-type">double</span> <span class="org-variable-name">m_gradient</span> = 0.0;
        <span class="org-type">double</span> <span class="org-variable-name">b_gradient</span> = 0.0;

        <span class="org-comment-delimiter">// </span><span class="org-comment">Calculate gradients</span>
        <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">i</span> = 0; i &lt; n; i++) {
            <span class="org-type">double</span> <span class="org-variable-name">prediction</span> = (*m) * x[i] + (*b);
            m_gradient += -2 * x[i] * (y[i] - prediction);
            b_gradient += -2 * (y[i] - prediction);
        }

        <span class="org-comment-delimiter">// </span><span class="org-comment">Update parameters</span>
        *m -= (m_gradient / n) * learningRate;
        *b -= (b_gradient / n) * learningRate;

        <span class="org-comment-delimiter">// </span><span class="org-comment">Optionally print the MSE every 100 iterations</span>
        <span class="org-keyword">if</span> (iter % 100 == 0) {
            printf(<span class="org-string">"Iteration %d: MSE = %f\n"</span>, iter, mean_squared_error(x, y, *m, *b, n));
        }
    }
}

<span class="org-type">int</span> <span class="org-function-name">main</span>() {
    <span class="org-comment-delimiter">// </span><span class="org-comment">Sample data points (x, y)</span>
    <span class="org-type">double</span> <span class="org-variable-name">x</span>[] = {1, 2, 3, 4, 5};  <span class="org-comment-delimiter">// </span><span class="org-comment">input</span>
    <span class="org-type">double</span> <span class="org-variable-name">y</span>[] = {2, 3, 5, 7, 11}; <span class="org-comment-delimiter">// </span><span class="org-comment">target to predict</span>
    <span class="org-type">int</span> <span class="org-variable-name">n</span> = <span class="org-keyword">sizeof</span>(x) / <span class="org-keyword">sizeof</span>(x[0]); <span class="org-comment-delimiter">// </span><span class="org-comment">Number of data points</span>

    <span class="org-comment-delimiter">// </span><span class="org-comment">Initial parameters (y = mx + b)</span>
    <span class="org-type">double</span> <span class="org-variable-name">m</span> = 0.0; <span class="org-comment-delimiter">// </span><span class="org-comment">Slope</span>
    <span class="org-type">double</span> <span class="org-variable-name">b</span> = 0.0; <span class="org-comment-delimiter">// </span><span class="org-comment">Intercept</span>
    <span class="org-type">double</span> <span class="org-variable-name">learningRate</span> = 0.01;
    <span class="org-type">int</span> <span class="org-variable-name">iterations</span> = 1000;

    gradientDescent(x, y, &amp;m, &amp;b, n, learningRate, iterations);
    printf(<span class="org-string">"Final parameters: m = %f, b = %f\n"</span>, m, b);
    <span class="org-keyword">return</span> 0;
}
</pre>
</div>
</div>
</div>
</main>
</body>
</html>
